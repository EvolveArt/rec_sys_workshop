{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pGi1CZsoogZ"
   },
   "source": [
    "# Bayesian Personalized Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AicJk-HcqGZm"
   },
   "source": [
    "Il s'agit d'un algorithme implicite, c'est une approche qui donne les 'rankings' pour un jeu d'items pour un utilisateur spécifique.\n",
    "\n",
    "BPR s'intéresse au triplet (u, i, j) avec ***u*** un utilisateur, ***i*** un item connu et ***j*** un item inconnu.\n",
    "\n",
    "BPR utilise une formule bayésienne pour maximiser la probabilité postérieure. (Formule de Bayes)\n",
    "\n",
    "---\n",
    "\n",
    "Le critère d'optimisation final est le suivant :\n",
    "\n",
    "\n",
    "![Texte alternatif…](https://drive.google.com/uc?id=1EJ5cSOEoKdlaCWg0SM0OEhvs7yH9LeGg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KwQSHhytm6D"
   },
   "source": [
    "### Modèle Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "o0-IiQgIou-7",
    "outputId": "364f676d-2235-460d-e1a1-03b98a703f24"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwaXO5YMuoOV"
   },
   "source": [
    "Preparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTS3KT2AzUTT"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00000c289a1829a808ac09c00daf10bc3c4e223b</th>\n",
       "      <th>3bd73256-3905-4f3a-97e2-8b341527f805</th>\n",
       "      <th>betty blowtorch</th>\n",
       "      <th>2137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>f2fb0ff0-5679-42ec-a55c-15109ce6e320</td>\n",
       "      <td>die Ärzte</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>b3ae82c2-e60b-4551-a76d-6620f1b456aa</td>\n",
       "      <td>melissa etheridge</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>3d6bbeb7-f90e-4d10-b440-e153c0d10b53</td>\n",
       "      <td>elvenking</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8</td>\n",
       "      <td>juliette &amp; the licks</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>8bfac288-ccc5-448d-9573-c33ea2aa5c30</td>\n",
       "      <td>red hot chili peppers</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   00000c289a1829a808ac09c00daf10bc3c4e223b  \\\n",
       "0  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "1  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "2  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "3  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "4  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "\n",
       "   3bd73256-3905-4f3a-97e2-8b341527f805        betty blowtorch  2137  \n",
       "0  f2fb0ff0-5679-42ec-a55c-15109ce6e320              die Ärzte  1099  \n",
       "1  b3ae82c2-e60b-4551-a76d-6620f1b456aa      melissa etheridge   897  \n",
       "2  3d6bbeb7-f90e-4d10-b440-e153c0d10b53              elvenking   717  \n",
       "3  bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8   juliette & the licks   706  \n",
       "4  8bfac288-ccc5-448d-9573-c33ea2aa5c30  red hot chili peppers   691  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['user', 'artist', 'plays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['user'].astype('category').cat.codes\n",
    "df['artist_id'] = df['artist'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_lookup = df[['artist_id', 'artist']].drop_duplicates()\n",
    "item_lookup['artist_id'] = item_lookup[\"artist_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['user', 'artist'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.plays != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.sort(df.user_id.unique()))\n",
    "artists = list(np.sort(df.artist_id.unique()))\n",
    "plays = list(df.plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.user_id.astype(float)\n",
    "cols = df.artist_id.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = sp.csr_matrix((plays, (rows, cols)), shape=(len(users), len(artists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity : 0.017 %\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1 - data_sparse.nnz / (data_sparse.shape[0] * data_sparse.shape[1])\n",
    "print(f'Sparsity : {sparsity*100:0.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids, iids = data_sparse.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batches = 30\n",
    "num_factors = 64 # Latent factors\n",
    "\n",
    "# Regularization parameters\n",
    "lambda_user = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "lambda_bias = 0.0000001\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.005\n",
    "\n",
    "# Triplets by batch\n",
    "samples = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variable(size, dim, name=None):\n",
    "    '''\n",
    "    Helper function to init a new variable with uniform random valuers\n",
    "    '''\n",
    "    std = np.sqrt(2/dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(inputs, size, dim, name=None):\n",
    "    '''\n",
    "    Helper function to get a Tensorflow variable and create an embedding lookup \n",
    "    in order to map our user and item indices to vector\n",
    "    '''\n",
    "    emb = init_variable(size, dim, name)\n",
    "    return tf.nn.embedding_lookup(emb, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(graph, session, name):\n",
    "    '''\n",
    "    Helper function to get the value of Tensorflow variable by name\n",
    "    '''\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    '''\n",
    "    Loss function: \n",
    "    -SUM ln σ(xui - xuj) + λ(w1)**2 + λ(w2)**2 + λ(w3)**2 ...\n",
    "    ln = the natural log\n",
    "    σ(xuij) = the sigmoid function of xuij.\n",
    "    λ = lambda regularization value.\n",
    "    ||W||**2 = the squared L2 norm of our model parameters.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Input into our model, in this case our user (u),\n",
    "    # known item (i) an unknown item (i) triplets.\n",
    "    u = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    i = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    j = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "\n",
    "    # User feature embedding\n",
    "    u_factors = embed(u, len(users), num_factors, 'user_factors') # U matrix\n",
    "\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(artists), num_factors, \"item_factors\") # V matrix\n",
    "    i_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "\n",
    "    # i and j bias embeddings.\n",
    "    item_bias = init_variable(len(artists), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.reshape(i_bias, [-1, 1])\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.reshape(j_bias, [-1, 1])\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown\n",
    "    # item to get xui and xuj.\n",
    "    xui = i_bias + tf.reduce_sum(u_factors * i_factors, axis=2)\n",
    "    xuj = j_bias + tf.reduce_sum(u_factors * j_factors, axis=2)\n",
    "\n",
    "    # We calculate xuij.\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve).\n",
    "    # if xuij is greater than 0, that means that \n",
    "    # xui is greater than xuj (and thats what we want).\n",
    "    u_auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "    # Output the AUC value to tensorboard for monitoring.\n",
    "    tf.summary.scalar('auc', u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by λ.\n",
    "    l2_norm = tf.add_n([\n",
    "        lambda_user * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias))\n",
    "        ])\n",
    "\n",
    "    # Calculate the loss as ||W||**2 - ln σ(Xuij)\n",
    "    #loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "\n",
    "    # Train using the Adam optimizer to minimize \n",
    "    # our loss function.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss)\n",
    "\n",
    "    # Initialize all tensorflow variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0 : Loss: 0.797 | AUC: 0.530\n",
      " Epoch 1 : Loss: 0.750 | AUC: 0.566\n",
      " Epoch 2 : Loss: 0.696 | AUC: 0.609\n",
      " Epoch 3 : Loss: 0.651 | AUC: 0.635\n",
      " Epoch 4 : Loss: 0.606 | AUC: 0.672\n",
      " Epoch 5 : Loss: 0.564 | AUC: 0.704\n",
      " Epoch 6 : Loss: 0.523 | AUC: 0.734\n",
      " Epoch 7 : Loss: 0.480 | AUC: 0.769\n",
      " Epoch 8 : Loss: 0.442 | AUC: 0.799\n",
      " Epoch 9 : Loss: 0.394 | AUC: 0.836\n",
      " Epoch 10 : Loss: 0.357 | AUC: 0.860\n",
      " Epoch 11 : Loss: 0.314 | AUC: 0.884\n",
      " Epoch 12 : Loss: 0.280 | AUC: 0.900\n",
      " Epoch 13 : Loss: 0.246 | AUC: 0.915\n",
      " Epoch 14 : Loss: 0.224 | AUC: 0.928\n",
      " Epoch 15 : Loss: 0.206 | AUC: 0.931\n",
      " Epoch 16 : Loss: 0.181 | AUC: 0.945\n",
      " Epoch 17 : Loss: 0.174 | AUC: 0.948\n",
      " Epoch 18 : Loss: 0.160 | AUC: 0.952\n",
      " Epoch 19 : Loss: 0.149 | AUC: 0.956\n",
      " Epoch 20 : Loss: 0.142 | AUC: 0.959\n",
      " Epoch 21 : Loss: 0.135 | AUC: 0.961\n",
      " Epoch 22 : Loss: 0.132 | AUC: 0.961\n",
      " Epoch 23 : Loss: 0.129 | AUC: 0.962\n",
      " Epoch 24 : Loss: 0.125 | AUC: 0.962\n",
      " Epoch 25 : Loss: 0.118 | AUC: 0.969\n",
      " Epoch 26 : Loss: 0.114 | AUC: 0.969\n",
      " Epoch 27 : Loss: 0.115 | AUC: 0.968\n",
      " Epoch 28 : Loss: 0.112 | AUC: 0.969\n",
      " Epoch 29 : Loss: 0.109 | AUC: 0.970\n",
      " Epoch 30 : Loss: 0.111 | AUC: 0.969\n",
      " Epoch 31 : Loss: 0.104 | AUC: 0.973\n",
      " Epoch 32 : Loss: 0.104 | AUC: 0.972\n",
      " Epoch 33 : Loss: 0.102 | AUC: 0.973\n",
      " Epoch 34 : Loss: 0.100 | AUC: 0.975\n",
      " Epoch 35 : Loss: 0.105 | AUC: 0.973\n",
      " Epoch 36 : Loss: 0.095 | AUC: 0.976\n",
      " Epoch 37 : Loss: 0.094 | AUC: 0.975\n",
      " Epoch 38 : Loss: 0.093 | AUC: 0.976\n",
      " Epoch 39 : Loss: 0.099 | AUC: 0.974\n",
      " Epoch 40 : Loss: 0.095 | AUC: 0.975\n",
      " Epoch 41 : Loss: 0.092 | AUC: 0.977\n",
      " Epoch 42 : Loss: 0.093 | AUC: 0.976\n",
      " Epoch 43 : Loss: 0.094 | AUC: 0.978\n",
      " Epoch 44 : Loss: 0.090 | AUC: 0.978\n",
      " Epoch 45 : Loss: 0.087 | AUC: 0.979\n",
      " Epoch 46 : Loss: 0.088 | AUC: 0.980\n",
      " Epoch 47 : Loss: 0.088 | AUC: 0.979\n",
      " Epoch 48 : Loss: 0.090 | AUC: 0.978\n",
      " Epoch 49 : Loss: 0.090 | AUC: 0.977\n"
     ]
    }
   ],
   "source": [
    "# Run the session. \n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    # This has noting to do with tensorflow but gives\n",
    "    # us a nice progress bar for the training.\n",
    "    # progress = tqdm(total=batches*epochs)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batches):\n",
    "\n",
    "            # We want to sample one known and one unknown \n",
    "            # item for each user. \n",
    "            import random\n",
    "            idx = random.sample(range(len(uids)), samples)\n",
    "\n",
    "            batch_u = [[uids[idxx]] for idxx in idx]\n",
    "            batch_i = [[iids[idxx]] for idxx in idx]\n",
    "\n",
    "            idx = random.sample(range(len(artists)), samples)\n",
    "            batch_j = [[idxx] for idxx in idx]\n",
    "\n",
    "            # Feed our users, known and unknown items to\n",
    "            # our tensorflow graph. \n",
    "            feed_dict = { u: batch_u, i: batch_i, j: batch_j }\n",
    "\n",
    "            # We run the session.\n",
    "            _, l, auc = sess.run([step, loss, u_auc], feed_dict)\n",
    "\n",
    "        # progress.update(batches)\n",
    "        print(' Epoch %d : Loss: %.3f | AUC: %.3f' % (epoch, l, auc))\n",
    "        \n",
    "    saver.save(sess, 'models/bpr-recommender-0.1')\n",
    "\n",
    "# progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 artist     score\n",
      "0       danny fernandes  8.407548\n",
      "1               beyoncé  8.157237\n",
      "2                  hoku  7.960122\n",
      "3                 ciara  7.907829\n",
      "4         janet jackson  7.778776\n",
      "5                cassie  7.777456\n",
      "6  gaiola das popozudas  7.659938\n",
      "7           brit & alex  7.657467\n",
      "8             m. pokora  7.651463\n",
      "9              deepside  7.610708\n"
     ]
    }
   ],
   "source": [
    "def find_similar_artists(artist=None, num_items=10):\n",
    "    \"\"\"Find artists similar to an artist.\n",
    "    Args:\n",
    "        artist (str): The name of the artist we want to find similar artists for\n",
    "        num_items (int): How many similar artists we want to return.\n",
    "    Returns:\n",
    "        similar (pandas.DataFrame): DataFrame with num_items artist names and scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab our User matrix U\n",
    "    user_vecs = get_variable(graph, session, 'user_factors')\n",
    "\n",
    "    # Grab our Item matrix V\n",
    "    item_vecs = get_variable(graph, session, 'item_factors')\n",
    "\n",
    "    # Grab our item bias\n",
    "    item_bi = get_variable(graph, session, 'item_bias').reshape(-1)\n",
    "\n",
    "    # Get the item id for Lady GaGa\n",
    "    item_id = int(item_lookup[item_lookup.artist == artist]['artist_id'])\n",
    "\n",
    "    # Get the item vector for our item_id and transpose it.\n",
    "    item_vec = item_vecs[item_id].T\n",
    "\n",
    "    # Calculate the similarity between Lady GaGa and all other artists\n",
    "    # by multiplying the item vector with our item_matrix\n",
    "    scores = np.add(item_vecs.dot(item_vec), item_bi).reshape(1,-1)[0]\n",
    "\n",
    "    # Get the indices for the top 10 scores\n",
    "    top_10 = np.argsort(scores)[::-1][:num_items]\n",
    "\n",
    "    # We then use our lookup table to grab the names of these indices\n",
    "    # and add it along with its score to a pandas dataframe.\n",
    "    artists, artist_scores = [], []\n",
    "    \n",
    "    for idx in top_10:\n",
    "        artists.append(item_lookup.artist.loc[item_lookup.artist_id == str(idx)].iloc[0])\n",
    "        artist_scores.append(scores[idx])\n",
    "\n",
    "    similar = pd.DataFrame({'artist': artists, 'score': artist_scores})\n",
    "\n",
    "    return similar\n",
    "\n",
    "print(find_similar_artists(artist='beyoncé'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             artist     score\n",
      "0      mamá ladilla  3.993062\n",
      "1        les wampas  3.982560\n",
      "2            eiffel  3.948097\n",
      "3  os cascavelletes  3.891437\n",
      "4     oliver onions  3.879301\n",
      "5             narco  3.836826\n",
      "6             trust  3.815100\n",
      "7        f.r. david  3.811432\n",
      "8        gramofocas  3.794619\n",
      "9          gigatron  3.759100\n"
     ]
    }
   ],
   "source": [
    "def make_recommendation(user_id=None, num_items=10):\n",
    "    \"\"\"Recommend items for a given user given a trained model\n",
    "    Args:\n",
    "        user_id (int): The id of the user we want to create recommendations for.\n",
    "        num_items (int): How many recommendations we want to return.\n",
    "    Returns:\n",
    "        recommendations (pandas.DataFrame): DataFrame with num_items artist names and scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab our user matrix U\n",
    "    user_vecs = get_variable(graph, session, 'user_factors')\n",
    "\n",
    "    # Grab our item matrix V\n",
    "    item_vecs = get_variable(graph, session, 'item_factors')\n",
    "\n",
    "    # Grab our item bias\n",
    "    item_bi = get_variable(graph, session, 'item_bias').reshape(-1)\n",
    "\n",
    "    # Calculate the score for our user for all items. \n",
    "    rec_vector = np.add(user_vecs[user_id, :].dot(item_vecs.T), item_bi)\n",
    "\n",
    "    # Grab the indices of the top users\n",
    "    item_idx = np.argsort(rec_vector)[::-1][:num_items]\n",
    "\n",
    "    # Map the indices to artist names and add to dataframe along with scores.\n",
    "    artists, scores = [], []\n",
    "\n",
    "    for idx in item_idx:\n",
    "        artists.append(item_lookup.artist.loc[item_lookup.artist_id == str(idx)].iloc[0])\n",
    "        scores.append(rec_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({'artist': artists, 'score': scores})\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "print(make_recommendation(user_id=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "BPR_recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "reco",
   "language": "python",
   "name": "reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}